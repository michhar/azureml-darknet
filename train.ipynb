{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Impressions](https://PixelServer20190423114238.azurewebsites.net/api/impressions/MachineLearningNotebooks/how-to-use-azureml/ml-frameworks/fastai/train-with-custom-docker/fastai-with-custom-docker.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a model using a custom Docker image and Darknet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, learn how to use a custom Docker image when training models with Azure Machine Learning and leverage the Darknet framework."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "1. Install of Python 3 in development environment (e.g. local or DSVM).  Use `pip install requirements_local.txt` to install necessary packages on the command line in a Python environment (virtual or conda environment).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Workspace\n",
    "from azureml.core import Dataset\n",
    "from azureml.core import Environment\n",
    "from azureml.core.compute import ComputeTarget, AmlCompute\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "from azureml.core import ScriptRunConfig\n",
    "from azureml.core import Experiment\n",
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "\n",
    "import os\n",
    "from uuid import uuid4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize a workspace\n",
    "The Azure Machine Learning workspace is the top-level resource for the service. It provides you with a centralized place to work with all the artifacts you create. In the Python SDK, you can access the workspace artifacts by creating a `workspace` object.\n",
    "\n",
    "Create a workspace object from the `config.json` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "ws = Workspace.from_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload dataset to default Data Store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Default data store (backend is Blob Storage) for this Workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "workspaceblobstore AzureBlob michharazureml2560334554 azureml-blobstore-35a51bb6-f732-44af-a8b9-4b773a603065\n"
     ]
    }
   ],
   "source": [
    "# Get the default datastore\n",
    "ds = ws.get_default_datastore()\n",
    "print(ds.name, ds.datastore_type, ds.account_name, ds.container_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upload the zipped data folder where the `data` folder, unzipped, is structured as:\n",
    "```\n",
    "    data/\n",
    "        img/\n",
    "            image1.jpg\n",
    "            image1.txt\n",
    "            image2.jpg\n",
    "            image2.txt\n",
    "            ...\n",
    "        train.txt\n",
    "        valid.txt\n",
    "        obj.data\n",
    "        obj.names\n",
    "```\n",
    "\n",
    "Note, `train.txt` looks similiar to the following snippet (image path is from `data` root) and `valid.txt` follows the same pattern:\n",
    "```\n",
    "data/img/image1.jpg\n",
    "data/img/image2.jpg\n",
    "...\n",
    "```\n",
    "\n",
    "Note, it is recommended that 5-10% of all images should go in to the `valid.txt` image list.  There should not be overlap between the two lists.\n",
    "\n",
    "Zip this folder and call it `data.zip` or change value below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading an estimated of 1 files\n",
      "Uploading data.zip\n",
      "Uploaded data.zip, 1 files out of an estimated total of 1\n",
      "Uploaded 1 files\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "$AZUREML_DATAREFERENCE_e3d7aad32d7841439350a6a1ab6b5d18"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Upload zip file\n",
    "ds.upload_files(['data.zip'], target_path='data', overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an Azure ML Dataset.  A Dataset can reference single or multiple files in your datastores or public urls. The files can be of any format. Dataset provides you with the ability to download or mount the files to your compute. By creating a dataset, you create a reference to the data source location. The data remains in its existing location, so no extra storage cost is incurred."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize file dataset \n",
    "ds_paths = [(ds, 'data/')]\n",
    "dataset = Dataset.File.from_files(path=ds_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare scripts\n",
    "Create a directory titled `darknet_scripts` for training script any any testing scripts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('darknet_scripts', exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup test script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then run the cell below to create the a script to test the setup in the directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting darknet_scripts/test_setup.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile darknet_scripts/test_setup.py\n",
    "\"\"\"\n",
    "Azure ML test setup script for Darknet object detection experiment\n",
    "\"\"\"\n",
    "import os\n",
    "import requests\n",
    "import subprocess\n",
    "import shutil\n",
    "import argparse\n",
    "\n",
    "\n",
    "# Test greeting\n",
    "def greeting():\n",
    "    print(\"Welcome to darknet container!\")\n",
    "greeting()\n",
    "\n",
    "os.makedirs('./outputs', exist_ok=True)\n",
    "\n",
    "# Arguments\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--data-folder', type=str,\n",
    "                    dest='data_folder', help='data folder')\n",
    "args = parser.parse_args()\n",
    "\n",
    "# Look at data folder\n",
    "print('Data folder is at:', args.data_folder)\n",
    "print('List all files: ', os.listdir(args.data_folder))\n",
    "\n",
    "zipfilename = os.path.join(args.data_folder, \"data.zip\")\n",
    "# Unzip the data.zip to cwd where it will be a folder called \"data\"\n",
    "shutil.unpack_archive(zipfilename, \".\")\n",
    "fulldatapath = os.path.join(args.data_folder, \"data\")\n",
    "\n",
    "print(\"Contents of data folder: \")\n",
    "os.system(\"ls data\")\n",
    "\n",
    "print(\"Getting the test image...\")\n",
    "# Get a test image\n",
    "url = \"https://raw.githubusercontent.com/AlexeyAB/darknet/master/data/giraffe.jpg\"\n",
    "response = requests.get(url)\n",
    "if response.status_code == 200:\n",
    "    with open(\"giraffe.jpg\", \"wb\") as f:\n",
    "        f.write(response.content)\n",
    "\n",
    "# Make coco.data file\n",
    "coco_data = \"\"\"\n",
    "classes= 80\n",
    "train  = train.txt\n",
    "valid  = val.txt\n",
    "names = coco.names\n",
    "backup = backup/\n",
    "eval=coco\n",
    "\"\"\"\n",
    "print(\"Making coco.data\")\n",
    "with open(\"coco.data\", \"w\") as f:\n",
    "    f.write(coco_data)\n",
    "\n",
    "print(\"Getting coco.names...\")\n",
    "# Get the names file\n",
    "url = \"https://raw.githubusercontent.com/AlexeyAB/darknet/master/cfg/coco.names\"\n",
    "response = requests.get(url)\n",
    "if response.status_code == 200:\n",
    "    with open(\"coco.names\", \"wb\") as f:\n",
    "        f.write(response.content)\n",
    "\n",
    "print(\"Getting the weights file, yolov4.weights...\")\n",
    "# Get the weights file\n",
    "url = \"https://github.com/AlexeyAB/darknet/releases/download/darknet_yolo_v3_optimal/yolov4.weights\"\n",
    "response = requests.get(url)\n",
    "if response.status_code == 200:\n",
    "    with open(\"yolov4.weights\", \"wb\") as f:\n",
    "        f.write(response.content)\n",
    "\n",
    "print(\"Getting the config file...\")\n",
    "# Get the config file\n",
    "url = \"https://raw.githubusercontent.com/AlexeyAB/darknet/master/cfg/yolov4.cfg\"\n",
    "response = requests.get(url)\n",
    "if response.status_code == 200:\n",
    "    with open(\"yolov4.cfg\", \"wb\") as f:\n",
    "        f.write(response.content)\n",
    "\n",
    "# What is our current working directory?\n",
    "print(\"Current working directory: {}\".format(os.getcwd()))\n",
    "print(\"Contents of directory: \")\n",
    "os.system(\"ls\")\n",
    "\n",
    "# Predict with darknet\n",
    "print(\"Running darknet detector test!\")\n",
    "os.system(\"darknet detector test coco.data yolov4.cfg yolov4.weights -thresh 0.25 giraffe.jpg -ext_output\")\n",
    "os.system(\"ls\")\n",
    "\n",
    "if os.path.exists(\"predictions.jpg\"):\n",
    "    shutil.copyfile(\"predictions.jpg\", \"outputs/predictions.jpg\")\n",
    "if os.path.exists(\"predictions.png\"):\n",
    "    shutil.copyfile(\"predictions.png\", \"outputs/predictions.png\")\n",
    "\n",
    "\n",
    "print(\"Return value: {}\".format(retval))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train script\n",
    "\n",
    "This is the training script.  It need not be modified as it utilizes some of the variables above as arguments, however feel free to improve it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting darknet_scripts/train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile darknet_scripts/train.py\n",
    "\"\"\"\n",
    "Azure ML train script for Darknet object detection experiment\n",
    "\"\"\"\n",
    "import os\n",
    "import requests\n",
    "import subprocess\n",
    "import shutil\n",
    "import argparse\n",
    "\n",
    "\n",
    "# Fill in number of classes\n",
    "num_classes = 1\n",
    "\n",
    "# Fill in with a list of anchor boxes\n",
    "anchors = \"11, 33,  18, 28,  19, 33,  22, 30,  22, 34,  25, 34\"\n",
    "\n",
    "# Test greeting\n",
    "def greeting():\n",
    "    print(\"Welcome to darknet container!\")\n",
    "greeting()\n",
    "\n",
    "os.makedirs('./outputs', exist_ok=True)\n",
    "\n",
    "# Arguments\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--data-folder', type=str,\n",
    "                    dest='data_folder', help='data folder')\n",
    "parser.add_argument('--lr', type=float, default=0.001,\n",
    "                    dest='lr', help='learning rate')\n",
    "parser.add_argument('--bs', type=int, default=4,\n",
    "                    dest='bs', help='minibatch size')\n",
    "parser.add_argument('--epochs', type=int, default=4,\n",
    "                    dest='epochs', help='number of epochs')\n",
    "args = parser.parse_args()\n",
    "\n",
    "# ========================== Get data ==========================\n",
    "\n",
    "# Look at data folder\n",
    "print('Data folder is at:', args.data_folder)\n",
    "print('List all files: ', os.listdir(args.data_folder))\n",
    "\n",
    "zipfilename = os.path.join(args.data_folder, \"data.zip\")\n",
    "# Unzip the data.zip to cwd where it will be a folder called \"data\"\n",
    "shutil.unpack_archive(zipfilename, \".\")\n",
    "fulldatapath = os.path.join(args.data_folder, \"data\")\n",
    "\n",
    "# ========================== Create or download necessary files ==========================\n",
    "\n",
    "# Make obj.data file (if we put model in \"outputs\" it will show in the Portal)\n",
    "obj_data = \"\"\"\n",
    "classes= {}\n",
    "train  = data/train.txt\n",
    "valid  = data/valid.txt\n",
    "names = data/obj.names\n",
    "backup = outputs/\n",
    "\"\"\".format(num_classes)\n",
    "\n",
    "print(\"Making obj.data...\") # see top of notebook\n",
    "with open(\"obj.data\", \"w\") as f:\n",
    "    f.write(obj_data)\n",
    "    \n",
    "# Get the pre-trained weights file\n",
    "print(\"Getting the pre-trained weights file, yolov4-tiny.conv.29...\")\n",
    "url = \"https://github.com/AlexeyAB/darknet/releases/download/darknet_yolo_v4_pre/yolov4-tiny.conv.29\"\n",
    "response = requests.get(url)\n",
    "if response.status_code == 200:\n",
    "    with open(\"yolov4-tiny.conv.29\", \"wb\") as f:\n",
    "        f.write(response.content)\n",
    "\n",
    "print(\"Creating the config file...\")\n",
    "# Get the config file\n",
    "url = \"https://raw.githubusercontent.com/AlexeyAB/darknet/master/cfg/yolov4-tiny.cfg\"\n",
    "response = requests.get(url)\n",
    "if response.status_code == 200:\n",
    "    with open(\"yolov4-tiny.cfg\", \"wb\") as f:\n",
    "        f.write(response.content)\n",
    "with open(\"yolov4-tiny.cfg\", \"r\") as f:\n",
    "    config_content = f.read()\n",
    "\n",
    "# Replace LR\n",
    "config_content = config_content.replace(\"learning_rate=0.00261\", \"learning_rate={}\".format(args.lr))\n",
    "# Replace number of filters in CN layer before yolo layer\n",
    "num_filters = (num_classes+5)*3\n",
    "config_content = config_content.replace(\"filters=255\", \"filters={}\".format(num_filters))\n",
    "# Replace number of classes\n",
    "config_content = config_content.replace(\"classes=80\", \"classes={}\".format(num_classes))\n",
    "# Replace batch size\n",
    "config_content = config_content.replace(\"batch=64\", \"batch={}\".format(args.bs))\n",
    "# Replace max batches/epochs and learning rate stepping epochs\n",
    "config_content = config_content.replace(\"max_batches = 2000200\", \"max_batches={}\".format(args.epochs))\n",
    "config_content = config_content.replace(\"steps=1600000,1800000\", \n",
    "                                        \"steps={},{}\".format(int(0.8*args.epochs), \n",
    "                                                             int(0.9*args.epochs)))\n",
    "# Replace anchors\n",
    "config_content = config_content.replace(\"10,14,  23,27,  37,58,  81,82,  135,169,  344,319\",\n",
    "                                       anchors)\n",
    "\n",
    "with open(\"yolov4-tiny-custom.cfg\", \"w\") as f:\n",
    "    f.write(config_content)\n",
    "# What does the config file look like now?\n",
    "os.system(\"cat yolov4-tiny-custom.cfg\")\n",
    "\n",
    "# What is our current working directory?\n",
    "print(\"Current working directory: {}\".format(os.getcwd()))\n",
    "print(\"Contents of directory: \")\n",
    "os.system(\"ls\")\n",
    "\n",
    "# ========================== Train model ==========================\n",
    "\n",
    "# Predict with darknet\n",
    "print(\"Running darknet training experiment for {} epochs!\".format(args.epochs))\n",
    "os.system(\"darknet detector train obj.data yolov4-tiny-custom.cfg yolov4-tiny.conv.29 -map -dont_show -clear\")\n",
    "os.system(\"ls\")\n",
    "\n",
    "# ========================== Evaluate model - TBD ==========================\n",
    "\n",
    "\n",
    "\n",
    "# ========================== Small test - optional ==========================\n",
    "\n",
    "# print(\"Getting the test image...\")\n",
    "# # Get a test image\n",
    "# url = \"https://raw.githubusercontent.com/AlexeyAB/darknet/master/data/giraffe.jpg\"\n",
    "# response = requests.get(url)\n",
    "# if response.status_code == 200:\n",
    "#     with open(\"giraffe.jpg\", \"wb\") as f:\n",
    "#         f.write(response.content)\n",
    "\n",
    "# # Predict with darknet\n",
    "# print(\"Running darknet detector test!\")\n",
    "# os.system(\"darknet detector test coco.data yolov4.cfg yolov4.weights -thresh 0.25 giraffe.jpg -ext_output\")\n",
    "# os.system(\"ls\")\n",
    "\n",
    "# if os.path.exists(\"predictions.jpg\"):\n",
    "#     shutil.copyfile(\"predictions.jpg\", \"outputs/predictions.jpg\")\n",
    "# if os.path.exists(\"predictions.png\"):\n",
    "#     shutil.copyfile(\"predictions.png\", \"outputs/predictions.png\")\n",
    "\n",
    "# ========================== Convert model to tflite ==========================\n",
    "\n",
    "# Set up project\n",
    "setup_tflite = \"\"\"\n",
    "    git clone https://github.com/hunglc007/tensorflow-yolov4-tflite.git\n",
    "\"\"\"\n",
    "\n",
    "os.system(setup_tflite)\n",
    "\n",
    "with open(\"tensorflow-yolov4-tflite/core/config.py\", \"r\") as f:\n",
    "    config_tflite = f.read()\n",
    "config_tflite = config_tflite.replace(\"coco.names\", \"obj.names\")\n",
    "with open(\"tensorflow-yolov4-tflite/core/config.py\", \"w\") as f:\n",
    "    f.write(config_tflite)\n",
    "shutil.copy(\"data/obj.names\", \"tensorflow-yolov4-tflite/data/classes/obj.names\")\n",
    "# TODO: replace anchors\n",
    "\n",
    "# Save as TF\n",
    "os.system(\"cd tensorflow-yolov4-tflite && python save_model.py --weights ../outputs/yolov4-tiny-custom_best.weights --output ../outputs/yolov4-tiny-416-tflite --input_size 416 --model yolov4 --framework tflite --tiny\")\n",
    "# Convert to TFLite\n",
    "os.system(\"cd tensorflow-yolov4-tflite && python convert_tflite.py --weights ../outputs/yolov4-tiny-416-tflite --output ../outputs/yolov4-tiny-416-fp16.tflite --quantize_mode float16\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define your environment\n",
    "Create an environment object and enable Docker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "darknet_env = Environment(\"darknet\")\n",
    "darknet_env.docker.enabled = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This specified base image supports the darknet framework which allows for object detection deep learning capabilities. For more information, see the [darknet GitHub repo](https://github.com/AlexeyAB/darknet). \n",
    "\n",
    "When you are using your custom Docker image, you might already have your Python environment properly set up. In that case, set the `user_managed_dependencies` flag to True in order to leverage your custom image's built-in python environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "darknet_env.docker.base_image = None\n",
    "darknet_env.docker.base_dockerfile = \"./Dockerfile\"\n",
    "# darknet_env.python.user_managed_dependencies = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use an image from a private container registry that is not in your workspace, you must use `docker.base_image_registry` to specify the address of the repository as well as a username and password."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "fastai_env.docker.base_image_registry.address = \"myregistry.azurecr.io\"\n",
    "fastai_env.docker.base_image_registry.username = \"username\"\n",
    "fastai_env.docker.base_image_registry.password = \"password\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also possible to use a custom Dockerfile. Use this approach if you need to install non-Python packages as dependencies and remember to set the base image to None. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define Python packages as part of environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "conda_dep = CondaDependencies()\n",
    "conda_dep.add_pip_package(\"tensorflow-gpu==2.3.1\")\n",
    "conda_dep.add_pip_package(\"opencv-python\")\n",
    "conda_dep.add_pip_package(\"lxml\")\n",
    "conda_dep.add_pip_package(\"tqdm\")\n",
    "conda_dep.add_pip_package(\"absl-py\")\n",
    "conda_dep.add_pip_package(\"matplotlib\")\n",
    "conda_dep.add_pip_package(\"easydict\")\n",
    "conda_dep.add_pip_package(\"pillow\")\n",
    "\n",
    "\n",
    "# Adds dependencies to PythonSection of myenv\n",
    "darknet_env.python.conda_dependencies=conda_dep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create or attach existing AmlCompute\n",
    "You will need to create a [compute target](https://docs.microsoft.com/azure/machine-learning/service/concept-azure-machine-learning-architecture#compute-target) for training your model. In this tutorial, you create `AmlCompute` as your training compute resource.\n",
    "\n",
    "**Creation of AmlCompute takes approximately 5 minutes.** If the AmlCompute with that name is already in your workspace this code will skip the creation process.\n",
    "\n",
    "As with other Azure services, there are limits on certain resources (e.g. AmlCompute) associated with the Azure Machine Learning service. Please read [this article](https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-manage-quotas) on the default limits and how to request more quota."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing compute target.\n",
      "{'currentNodeCount': 0, 'targetNodeCount': 0, 'nodeStateCounts': {'preparingNodeCount': 0, 'runningNodeCount': 0, 'idleNodeCount': 0, 'unusableNodeCount': 0, 'leavingNodeCount': 0, 'preemptedNodeCount': 0}, 'allocationState': 'Steady', 'allocationStateTransitionTime': '2021-03-04T05:38:19.333000+00:00', 'errors': None, 'creationTime': '2020-06-24T22:33:16.808425+00:00', 'modifiedTime': '2020-06-24T22:33:32.849969+00:00', 'provisioningState': 'Succeeded', 'provisioningStateTransitionTime': None, 'scaleSettings': {'minNodeCount': 0, 'maxNodeCount': 5, 'nodeIdleTimeBeforeScaleDown': 'PT120S'}, 'vmPriority': 'Dedicated', 'vmSize': 'STANDARD_NC6'}\n"
     ]
    }
   ],
   "source": [
    "# choose a name for your cluster\n",
    "cluster_name = \"gpuforkeras\"\n",
    "\n",
    "try:\n",
    "    compute_target = ComputeTarget(workspace=ws, name=cluster_name)\n",
    "    print('Found existing compute target.')\n",
    "except ComputeTargetException:\n",
    "    print('Creating a new compute target...')\n",
    "    compute_config = AmlCompute.provisioning_configuration(vm_size='STANDARD_NC6',\n",
    "                                                           max_nodes=4)\n",
    "\n",
    "    # create the cluster\n",
    "    compute_target = ComputeTarget.create(ws, cluster_name, compute_config)\n",
    "\n",
    "    compute_target.wait_for_completion(show_output=True)\n",
    "\n",
    "# use get_status() to get a detailed status for the current AmlCompute\n",
    "print(compute_target.get_status().serialize())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a ScriptRunConfig and submit for training\n",
    "This ScriptRunConfig will configure your job for execution on the desired compute target.  Here we are looping over a list of hyperparameters.  Note, the concurrency will be limited by the number of compute nodes in our compute target.\n",
    "\n",
    "When a training run is submitted using a ScriptRunConfig object, the submit method returns an object of type ScriptRun. The returned ScriptRun object gives you programmatic access to information about the training run. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "hyperparams = {\"learning_rate\": [0.001,0.003],\n",
    "               \"batch_size\": [2,4]}\n",
    "epochs = 1000\n",
    "\n",
    "# Iterate over hyperparameters\n",
    "for lr in hyperparams[\"learning_rate\"]:\n",
    "    for bs in hyperparams[\"batch_size\"]:\n",
    "    \n",
    "        script_args = ['--data-folder', \n",
    "                       dataset.as_named_input('data').as_mount('/tmp/{}'.format(uuid4())),\n",
    "                       '--lr', lr,\n",
    "                       '--bs', bs,\n",
    "                       '--epochs', epochs]\n",
    "\n",
    "        darknet_config = ScriptRunConfig(source_directory='darknet_scripts',\n",
    "                                        script='train.py',\n",
    "                                        arguments=script_args,\n",
    "                                        compute_target=compute_target,\n",
    "                                        environment=darknet_env)\n",
    "\n",
    "        run = Experiment(ws,'darknet-custom-image-hyper4').submit(darknet_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "authors": [
   {
    "name": "sagopal"
   }
  ],
  "category": "training",
  "compute": [
   "AML Compute"
  ],
  "datasets": [
   "Oxford IIIT Pet"
  ],
  "deployment": [
   "None"
  ],
  "exclude_from_index": false,
  "framework": [
   "Pytorch"
  ],
  "friendly_name": "Train a model with a custom Docker image",
  "index_order": 1,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "tags": [
   "None"
  ],
  "task": "Train with custom Docker image"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
